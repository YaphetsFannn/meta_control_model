
 Trainnig Args: Namespace(eval_grad_steps=50, eval_iters=5, inner_grad_steps=1, inner_step_size=0.02, iterations=20000, log='./logs/maml_ik_log.txt', mode=None, n_shot=200, outer_step_size=0.001, seed=1)
MAML/Training/Loss/0.388086915016 0
MAML/Training/Loss/0.388473463058 1
MAML/Training/Loss/0.396016287804 2
MAML/Training/Loss/0.401230736077 3
MAML/Training/Loss/0.402107380629 4
MAML/Training/Loss/0.404047026237 5
MAML/Training/Loss/0.401390703235 6
MAML/Training/Loss/0.397523564845 7
MAML/Training/Loss/0.397544206513 8
MAML/Training/Loss/0.397257678509 9
MAML/Training/Loss/0.393360085379 10
MAML/Training/Loss/0.396743733188 11
MAML/Training/Loss/0.395763058846 12
MAML/Training/Loss/0.393561794077 13
MAML/Training/Loss/0.393207374016 14
MAML/Training/Loss/0.391456040367 15
MAML/Training/Loss/0.392252950809 16
MAML/Training/Loss/0.392158656319 17
MAML/Training/Loss/0.391428932391 18
MAML/Training/Loss/0.391722148955 19
MAML/Training/Loss/0.389463135174 20
MAML/Training/Loss/0.388476794416 21
MAML/Training/Loss/0.387469694666 22
MAML/Training/Loss/0.386906466881 23
MAML/Training/Loss/0.385988372087 24
MAML/Training/Loss/0.385574766764 25
MAML/Training/Loss/0.384447771752 26
MAML/Training/Loss/0.384385440605 27
MAML/Training/Loss/0.38279694997 28
MAML/Training/Loss/0.382762878736 29
MAML/Training/Loss/0.382377425125 30
MAML/Training/Loss/0.381397197582 31
MAML/Training/Loss/0.380200410973 32
MAML/Training/Loss/0.380009327917 33
MAML/Training/Loss/0.37953970279 34
MAML/Training/Loss/0.378434347775 35
MAML/Training/Loss/0.37791787837 36
MAML/Training/Loss/0.37685513716 37
MAML/Training/Loss/0.375967274721 38
MAML/Training/Loss/0.375452343225 39
MAML/Training/Loss/0.37432721388 40
MAML/Training/Loss/0.373622769259 41
MAML/Training/Loss/0.372897522949 42
MAML/Training/Loss/0.372085151212 43
MAML/Training/Loss/0.37232911706 44
MAML/Training/Loss/0.370877188444 45
MAML/Training/Loss/0.369867179876 46
MAML/Training/Loss/0.368776291857 47
MAML/Training/Loss/0.368148931678 48
MAML/Training/Loss/0.367831563592 49
MAML/Training/Loss/0.366114985358 50
MAML/Training/Loss/0.365714474481 51
MAML/Training/Loss/0.364790949169 52
MAML/Training/Loss/0.36467589449 53
MAML/Training/Loss/0.363504640189 54
MAML/Training/Loss/0.363025119369 55
MAML/Training/Loss/0.362144015442 56
MAML/Training/Loss/0.361274860131 57
MAML/Training/Loss/0.360302092261 58
MAML/Training/Loss/0.360061219434 59
MAML/Training/Loss/0.359270812156 60
MAML/Training/Loss/0.358411378053 61
MAML/Training/Loss/0.357673451163 62
MAML/Training/Loss/0.356837127265 63
MAML/Training/Loss/0.356089510184 64
MAML/Training/Loss/0.355586125815 65
MAML/Training/Loss/0.354972597645 66
MAML/Training/Loss/0.353988275633 67
MAML/Training/Loss/0.353317835884 68
MAML/Training/Loss/0.352670521566 69
MAML/Training/Loss/0.352236467096 70
MAML/Training/Loss/0.351449041069 71
MAML/Training/Loss/0.350607413543 72
MAML/Training/Loss/0.350002290832 73
MAML/Training/Loss/0.349380424023 74
MAML/Training/Loss/0.348932504654 75
MAML/Training/Loss/0.348391789662 76
MAML/Training/Loss/0.347996838353 77
MAML/Training/Loss/0.347261774238 78
MAML/Training/Loss/0.346963736936 79
MAML/Training/Loss/0.346438656398 80
MAML/Training/Loss/0.346035192144 81
MAML/Training/Loss/0.345668757393 82
MAML/Training/Loss/0.345214083649 83
MAML/Training/Loss/0.344597533941 84
MAML/Training/Loss/0.344421809912 85
MAML/Training/Loss/0.343747561827 86
MAML/Training/Loss/0.343212094293 87
MAML/Training/Loss/0.342635213458 88
MAML/Training/Loss/0.342134295371 89
MAML/Training/Loss/0.341429247109 90
MAML/Training/Loss/0.340927720847 91
MAML/Training/Loss/0.340320221968 92
MAML/Training/Loss/0.339559264766 93
MAML/Training/Loss/0.338817650205 94
MAML/Training/Loss/0.338379538432 95
MAML/Training/Loss/0.337822891203 96
MAML/Training/Loss/0.33743738228 97
MAML/Training/Loss/0.33699545487 98
MAML/Training/Loss/0.336325837791 99
MAML/Training/Loss/0.335891548183 100
MAML/Training/Loss/0.335327212834 101
MAML/Training/Loss/0.334665850065 102
MAML/Training/Loss/0.334025968554 103
MAML/Training/Loss/0.333601837896 104
MAML/Training/Loss/0.3331211545 105
MAML/Training/Loss/0.332710838596 106
MAML/Training/Loss/0.332219782637 107
MAML/Training/Loss/0.331716434234 108
MAML/Training/Loss/0.331058556708 109
MAML/Training/Loss/0.330768736042 110
MAML/Training/Loss/0.330165737654 111
MAML/Training/Loss/0.329547693086 112
MAML/Training/Loss/0.3289481826 113
MAML/Training/Loss/0.328388224529 114
MAML/Training/Loss/0.32790365512 115
MAML/Training/Loss/0.327370199179 116
MAML/Training/Loss/0.326834022746 117
MAML/Training/Loss/0.326283840823 118
MAML/Training/Loss/0.325767681499 119
MAML/Training/Loss/0.325362906229 120
MAML/Training/Loss/0.324718191365 121
MAML/Training/Loss/0.324268324467 122
MAML/Training/Loss/0.32380586466 123
MAML/Training/Loss/0.323347591281 124
MAML/Training/Loss/0.322796150451 125
MAML/Training/Loss/0.322380421317 126
MAML/Training/Loss/0.321744204825 127
MAML/Training/Loss/0.321399123955 128
MAML/Training/Loss/0.320871536411 129
MAML/Training/Loss/0.320320117929 130
MAML/Training/Loss/0.320012908226 131
MAML/Training/Loss/0.319630575494 132
MAML/Training/Loss/0.319069357305 133
MAML/Training/Loss/0.318693385985 134
MAML/Training/Loss/0.318295746625 135
MAML/Training/Loss/0.317816400202 136
MAML/Training/Loss/0.317252751731 137
MAML/Training/Loss/0.316593585297 138
MAML/Training/Loss/0.316133657353 139
MAML/Training/Loss/0.315764205929 140
MAML/Training/Loss/0.315301447729 141
MAML/Training/Loss/0.314828840836 142
MAML/Training/Loss/0.31424983626 143
MAML/Training/Loss/0.313679857439 144
MAML/Training/Loss/0.313376944094 145
MAML/Training/Loss/0.312906080403 146
MAML/Training/Loss/0.31250466793 147
MAML/Training/Loss/0.3119522994 148
MAML/Training/Loss/0.311483560681 149
MAML/Training/Loss/0.310990967695 150
MAML/Training/Loss/0.310500646696 151
MAML/Training/Loss/0.310113317242 152
MAML/Training/Loss/0.309679976435 153
MAML/Training/Loss/0.309224025453 154
MAML/Training/Loss/0.308835315762 155
MAML/Training/Loss/0.308331406971 156
MAML/Training/Loss/0.307937043754 157
MAML/Training/Loss/0.307419043854 158
MAML/Training/Loss/0.307013279479 159
MAML/Training/Loss/0.30659777324 160
MAML/Training/Loss/0.306132442789 161
MAML/Training/Loss/0.305705563642 162
MAML/Training/Loss/0.305146178585 163
MAML/Training/Loss/0.304860333909 164
MAML/Training/Loss/0.304450541681 165
MAML/Training/Loss/0.30410365059 166
MAML/Training/Loss/0.303702418595 167

 Trainnig Args: Namespace(eval_grad_steps=50, eval_iters=5, inner_grad_steps=1, inner_step_size=0.02, iterations=2000, log='./logs/maml_ik_log.txt', mode=None, n_shot=200, outer_step_size=0.001, seed=1)
MAML/Training/Loss/0.385320347548 0
MAML/Training/Loss/0.336921843916 100
MAML/Training/Loss/0.292086743612 200
MAML/Training/Loss/0.260181873502 300
MAML/Training/Loss/0.235370873862 400
MAML/Training/Loss/0.216223047772 500
MAML/Training/Loss/0.200876435828 600
MAML/Training/Loss/0.188628904564 700
MAML/Training/Loss/0.178464144576 800
MAML/Training/Loss/0.170036377153 900
MAML/Training/Loss/0.162769143462 1000
MAML/Training/Loss/0.156716793393 1100
MAML/Training/Loss/0.151538595142 1200
MAML/Training/Loss/0.14698438448 1300
MAML/Training/Loss/0.143042038699 1400
MAML/Training/Loss/0.13954750067 1500
MAML/Training/Loss/0.136413504221 1600
MAML/Training/Loss/0.13363827968 1700
MAML/Training/Loss/0.13115958936 1800
MAML/Training/Loss/0.128938896827 1900

 Trainnig Args: Namespace(eval_grad_steps=50, eval_iters=5, inner_grad_steps=1, inner_step_size=0.02, iterations=2000, log='./logs/maml_ik_log.txt', mode=None, n_shot=200, outer_step_size=0.001, seed=1)
MAML/Training/Loss/0.363643205166 0
MAML/Training/Loss/0.336295725213 100
MAML/Training/Loss/0.293576608947 200
MAML/Training/Loss/0.261089060374 300
MAML/Training/Loss/0.236157443027 400
MAML/Training/Loss/0.216822621402 500
MAML/Training/Loss/0.201077180302 600
MAML/Training/Loss/0.18859757517 700
MAML/Training/Loss/0.178416278021 800
MAML/Training/Loss/0.169980984738 900
MAML/Training/Loss/0.162935073922 1000
MAML/Training/Loss/0.156830023838 1100
MAML/Training/Loss/0.151579757759 1200
MAML/Training/Loss/0.147026921672 1300
MAML/Training/Loss/0.143071212426 1400
MAML/Training/Loss/0.139619026452 1500
MAML/Training/Loss/0.136508997874 1600
MAML/Training/Loss/0.133655153854 1700
MAML/Training/Loss/0.131187912169 1800
MAML/Training/Loss/0.128932345472 1900

 Trainnig Args: Namespace(eval_grad_steps=50, eval_iters=5, inner_grad_steps=1, inner_step_size=0.02, iterations=2000, log='./logs/maml_ik_log.txt', mode=None, n_shot=200, outer_step_size=0.001, seed=1)
MAML/Training/Loss/0.386396491528 0

 Trainnig Args: Namespace(eval_grad_steps=50, eval_iters=5, inner_grad_steps=1, inner_step_size=0.02, iterations=2000, log='./logs/maml_ik_log.txt', mode=None, n_shot=200, outer_step_size=0.001, seed=1)
MAML/Training/Loss/0.38198825717 0
MAML/Training/Loss/0.335373364581 100
MAML/Training/Loss/0.292789683161 200
MAML/Training/Loss/0.260718604952 300
MAML/Training/Loss/0.236016994291 400
MAML/Training/Loss/0.216412926294 500
MAML/Training/Loss/0.200805711176 600
MAML/Training/Loss/0.188285363197 700
MAML/Training/Loss/0.178096315094 800
MAML/Training/Loss/0.169686915142 900
MAML/Training/Loss/0.162568233751 1000
MAML/Training/Loss/0.156446678708 1100
MAML/Training/Loss/0.151212563566 1200
MAML/Training/Loss/0.146752130369 1300
MAML/Training/Loss/0.142790845458 1400
MAML/Training/Loss/0.139337257426 1500
MAML/Training/Loss/0.136272593262 1600
MAML/Training/Loss/0.133466847753 1700
MAML/Training/Loss/0.130999644034 1800
MAML/Training/Loss/0.128784968924 1900
eval loss is 0.071908540247
eval loss is 0.0730936420354
eval loss is 0.0727076753926

 Trainnig Args: Namespace(eval_grad_steps=50, eval_iters=5, inner_grad_steps=1, inner_step_size=0.02, iterations=2000, log='./logs/maml_ik_log.txt', mode=None, n_shot=200, outer_step_size=0.001, seed=1)
MAML/Training/Loss/0.386328589916 0
MAML/Training/Loss/0.335964480839 100
MAML/Training/Loss/0.293014242785 200
MAML/Training/Loss/0.261184756334 300
MAML/Training/Loss/0.236219053963 400
MAML/Training/Loss/0.216617472056 500
MAML/Training/Loss/0.200989799861 600
MAML/Training/Loss/0.188609815012 700
MAML/Training/Loss/0.178338256 800
MAML/Training/Loss/0.169864780996 900
MAML/Training/Loss/0.162658675354 1000
MAML/Training/Loss/0.156517750447 1100
MAML/Training/Loss/0.151247470896 1200
MAML/Training/Loss/0.146716249532 1300
MAML/Training/Loss/0.142763689105 1400
MAML/Training/Loss/0.139293505586 1500
MAML/Training/Loss/0.136193661185 1600
MAML/Training/Loss/0.133408424984 1700
MAML/Training/Loss/0.130975488218 1800
MAML/Training/Loss/0.128741207339 1900
eval loss is 0.0757320281801
eval loss cmp is 0.0727618849809
eval loss is 0.0732117915546
eval loss cmp is 0.0731661567168
eval loss is 0.0730085975056
eval loss cmp is 0.0726704638631

 Trainnig Args: Namespace(eval_grad_steps=50, eval_iters=5, inner_grad_steps=1, inner_step_size=0.02, iterations=500, log='./logs/maml_ik_log.txt', mode=None, n_shot=50, outer_step_size=0.1, seed=1)
MAML/Training/Loss/0.40013422966 0
MAML/Training/Loss/0.0744677500117 100
MAML/Training/Loss/0.0613097944626 200
MAML/Training/Loss/0.0549915459292 300
MAML/Training/Loss/0.0513876339667 400

 Trainnig Args: Namespace(eval_grad_steps=50, eval_iters=5, inner_grad_steps=1, inner_step_size=0.02, iterations=500, log='./logs/maml_ik_log.txt', mode=None, model_name='500ep', n_shot=50, outer_step_size=0.1, seed=1)
MAML/Training/Loss/0.405750834942 0
MAML/Training/Loss/0.0757807553405 100
MAML/Training/Loss/0.0617501840774 200
MAML/Training/Loss/0.0554856232903 300
